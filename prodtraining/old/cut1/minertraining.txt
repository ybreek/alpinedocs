Alpine Product Training
=======================


== Overview

In the set of activities described in this document we are going to explore a subset of Alpine's rich feature set.
We will use Alpine to explore a moderately large data set using descriptive statistics.
Then we will create two different prediction models for this data set, using a question with a yes/no outcome.  
We will then evaluate these prediction models and compare them using a graph called an ROC.  

The data set consists of details on airline arrivals and departures at Chicago's O'Hare International Airport in the year 2008.
We will use this data set to create a predictive model one using Naive Bayes and another using Logistic Regression to predict the probability of a flight being delayed or alternately to answer the question "Will this flight be delayed?"

Before we set about this task we need to be able to reliably execute certain elementary atomic tasks that will be needed repeatedly.
These tasks are identified in Appendix A.  Please make sure you are familiar with these operations especially Alpine account name, password, and datasource properties for the datasource you will use.


== Data Exploration ==

In this stage we will import our data into an Alpine Data Miner DataSource. Then we will do an initial exploration and simple cleanup of the data in preparation for modeling.  This stage will introduce us to some of the visualization capabilities of Alpine.

==== Import data from csv file

* You should have with you a csv file called 2008_ORD.csv or a zipped version of this file 
* Expand the zip file if necessary
* Create an Alpine DataSource by importing this file (see Appendix A Elementary Tasks)


When done you should see the following icon in your workspace.

[NOTE]
Figure of DataSource icon 




==== Set Up Initial flow

Now we will use Alpine for an initial exploration of data.  We will set up a flow with multiple visualizations incrementally, one module at a 
time.  At the end of this phase we should have a flow that looks like this.




Once we have created a DataSource we will use it to do an intial exploration of the data. The very first thing we do, something
that is almost always the first step, is running Summary Statistics on the data.  The Summary Statistics module in Alpine Data Miner does an inventory and sanity check of the data provided by the Data Source.  It then produces a tabular report with the results of it's efforts.

===== Summary Statistics

We use the Summary Statistics module operating on data from the Data Source.

Create a flow with the following schematic (see Appendix B for schematic syntax)
.....................................
ORD 2008 |------> Summary Statistics 
.....................................


[NOTE]
Figure for flow

Then run the flow.

Once the messages show that the flow has completed running, click on the Summary Statistics module.
You will see the results as

[NOTE]
Figure with Summary Statistic run output 

Placeholder

[NOTE]
Question to reviewers - how much to discuss about the content of the output.  What is important to discuss?

===== Null Value Replacement

Summary statistics output will tell us how many null values there were and where.
We need to replace null values with some concrete value such as 0 for numeric columns and '' for character columns.
The Null Value Replacement module does this en masse over the whole Data Source.

We set the values in the dialog as follows

[NOTE]
Figure with dialog for setting replacement values

Schematic for flow.

Now we run the flow again and when the flow has completed the values will have been replaced.

We are now ready to do some exploratory visualization.


===== Box plot

Schematic for flow.

[NOTE]
Need property settings for plot

===== Scatterplot matrix

Schematic for flow.

[NOTE]
Need property settings for plot


When we are done we should have a flow in our workspace that looks like this

image:images/DataIngestDataExploration.png[Data Ingest and Data Exploration]


==== Import data from Carriers file

Similar to the ORD_2008 import but for carriers.txt
[NOTE]
Where do we get the carriers.txt file?
For that matter where do we get the zipped ORD_2008 file?


== Model Creation

==== Variable selection

Schematic for flow.

[NOTE]
Figure with Variable Selection flow
What variables do we pick here?

==== Random Sampling
Create two data sets by random sampling from original data.

Schematic for flow.

==== Training Set
[NOTE]
Do we explain what Training Set is?

Schematic for flow.

==== Validation Set
[NOTE]
Do we explain what Validation Set is?

Schematic for flow.

===== Logistic Regression
[NOTE]
Do we explain what Logistic Regression is?

Schematic for flow.

===== Naive Bayes
[NOTE]
Do we explain what Naive Bayes is?

Schematic for flow.


== Model Evaluation

==== ROC

Schematic for flow.

NB, Logreg, Validation Set as inputs

==== LogReg Predictor

Schematic for flow.

Variable Selection, Logreg as inputs



=== Appendix A Elementary Tasks  

* Log in to Alpine Miner
 
* Create a connection

* Create a Flow 

* Set properties on a module 

* Create a Data source by importing from a CSV file 

[NOTE]
Figure with properties dialog for Import File 


* Join two data sources 

* Run a flow 


== Appendix B (Schematic syntax)

* A Flow
..........................................
SourceModuleName |---> TargetModuleName
e.g. ORD2008 |---> Summary Statistics 
..........................................

